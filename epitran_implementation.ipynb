{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO1w8ncczFeOzwV5jiQOSmO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Abhilitcode/DaveAI_Internship/blob/main/epitran_implementation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install epitran"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o004A26k6kuN",
        "outputId": "fa8f8c6c-b51b-4320-c093-b01b9716780b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting epitran\n",
            "  Downloading epitran-1.25.1-py2.py3-none-any.whl.metadata (34 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from epitran) (71.0.4)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from epitran) (2024.9.11)\n",
            "Collecting panphon>=0.20 (from epitran)\n",
            "  Downloading panphon-0.21.2-py2.py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: marisa-trie in /usr/local/lib/python3.10/dist-packages (from epitran) (1.2.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from epitran) (2.32.3)\n",
            "Collecting jamo (from epitran)\n",
            "  Downloading jamo-0.4.1-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting g2pk (from epitran)\n",
            "  Downloading g2pK-0.9.4-py3-none-any.whl.metadata (7.5 kB)\n",
            "Collecting unicodecsv (from panphon>=0.20->epitran)\n",
            "  Downloading unicodecsv-0.14.1.tar.gz (10 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from panphon>=0.20->epitran) (6.0.2)\n",
            "Requirement already satisfied: numpy>=1.20.2 in /usr/local/lib/python3.10/dist-packages (from panphon>=0.20->epitran) (1.26.4)\n",
            "Requirement already satisfied: editdistance in /usr/local/lib/python3.10/dist-packages (from panphon>=0.20->epitran) (0.8.1)\n",
            "Collecting munkres (from panphon>=0.20->epitran)\n",
            "  Downloading munkres-1.1.4-py2.py3-none-any.whl.metadata (980 bytes)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from g2pk->epitran) (3.8.1)\n",
            "Collecting konlpy (from g2pk->epitran)\n",
            "  Downloading konlpy-0.6.0-py2.py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting python-mecab-ko (from g2pk->epitran)\n",
            "  Downloading python_mecab_ko-1.3.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->epitran) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->epitran) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->epitran) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->epitran) (2024.8.30)\n",
            "Collecting JPype1>=0.7.0 (from konlpy->g2pk->epitran)\n",
            "  Downloading JPype1-1.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from konlpy->g2pk->epitran) (4.9.4)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->g2pk->epitran) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->g2pk->epitran) (1.4.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->g2pk->epitran) (4.66.5)\n",
            "Collecting python-mecab-ko-dic (from python-mecab-ko->g2pk->epitran)\n",
            "  Downloading python_mecab_ko_dic-2.1.1.post2-py3-none-any.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from JPype1>=0.7.0->konlpy->g2pk->epitran) (24.1)\n",
            "Downloading epitran-1.25.1-py2.py3-none-any.whl (184 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.1/184.1 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading panphon-0.21.2-py2.py3-none-any.whl (75 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.4/75.4 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading g2pK-0.9.4-py3-none-any.whl (27 kB)\n",
            "Downloading jamo-0.4.1-py3-none-any.whl (9.5 kB)\n",
            "Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.4/19.4 MB\u001b[0m \u001b[31m62.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading munkres-1.1.4-py2.py3-none-any.whl (7.0 kB)\n",
            "Downloading python_mecab_ko-1.3.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (577 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m577.1/577.1 kB\u001b[0m \u001b[31m35.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading JPype1-1.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (488 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m488.6/488.6 kB\u001b[0m \u001b[31m36.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_mecab_ko_dic-2.1.1.post2-py3-none-any.whl (34.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.5/34.5 MB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: unicodecsv\n",
            "  Building wheel for unicodecsv (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for unicodecsv: filename=unicodecsv-0.14.1-py3-none-any.whl size=10745 sha256=08b2f4ba45988dd4fe29acfc112fab53baf1582e17ed018ce1fb7f3a79521f5b\n",
            "  Stored in directory: /root/.cache/pip/wheels/9c/ea/66/8e45247b09052a933eb1a680b7c64802298faba58aac9b346b\n",
            "Successfully built unicodecsv\n",
            "Installing collected packages: unicodecsv, python-mecab-ko-dic, munkres, jamo, python-mecab-ko, panphon, JPype1, konlpy, g2pk, epitran\n",
            "Successfully installed JPype1-1.5.0 epitran-1.25.1 g2pk-0.9.4 jamo-0.4.1 konlpy-0.6.0 munkres-1.1.4 panphon-0.21.2 python-mecab-ko-1.3.7 python-mecab-ko-dic-2.1.1.post2 unicodecsv-0.14.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "y1wQKVjswVN4"
      },
      "outputs": [],
      "source": [
        "import epitran"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "epi_turkish = epitran.Epitran('tur-Latn')"
      ],
      "metadata": {
        "id": "tNRlSBiK6eaN"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation: Here, you create an instance of the Epitran class, specifying the language-script code 'tur-Latn', which stands for Turkish using the Latin script.\n",
        "How It Works: The constructor of Epitran sets up the necessary rules and mappings for converting Turkish orthography (written form) to its phonetic representation in IPA (International Phonetic Alphabet).\n",
        "Outcome: The variable epi_turkish now holds an object that knows how to transliterate Turkish words into IPA."
      ],
      "metadata": {
        "id": "Lr0bwCz57ZQ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transliterate a Turkish word\n",
        "ipa_transcription = epi_turkish.transliterate(u'Düğün')\n",
        "print(ipa_transcription)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vkniaMvm7Z97",
        "outputId": "41c1f967-5d52-4514-d7fc-668b2b693f7e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dyɰyn\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "epi_hindi = epitran.Epitran('hin-Deva')"
      ],
      "metadata": {
        "id": "AO3e7dnK7tFf"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Transliterate a Hindi word\n",
        "#IPA transcription, allowing you to see how the Hindi words are phonetically represented.\n",
        "ipa_transcription = epi_hindi.transliterate('अंग्रेज़ी हिन्दी शब्दकोश')\n",
        "ipa_transcription1 = epi_hindi.transliterate('पागल')\n",
        "print(ipa_transcription)\n",
        "print(ipa_transcription1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5lkOTFvz7kS9",
        "outputId": "113288ed-6408-42d1-f1b5-c3f72e7007ee"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ŋɡreziː ɦindiː ʃəbdəkoʃ\n",
            "paːɡəl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize Epitran for Mandarin Chinese (Simplified)\n",
        "epi_mandarin = epitran.Epitran('cmn-Hans', cedict_file='/content/sample_data/cedict_1_0_ts_utf-8_mdbg.txt')"
      ],
      "metadata": {
        "id": "e4Ii02-A8rPF"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#same from fa script\n",
        "ipa_transcription = epi_mandarin.transliterate(u'关 服务 高端 产品 仍 处于 供不应求 的 局面')\n",
        "print(ipa_transcription)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w2dlnLxXBS7H",
        "outputId": "3211684a-840e-4fb9-bf5b-dc67c13ddd34"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "kuan fuu kaotuan ʈ͡ʂʰanpʰin ɻeŋ ʈ͡ʂʰuju koŋpuiŋt͡ɕʰiou ti t͡ɕymian\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tuples = epi_turkish.word_to_tuples(u'Düğün')\n",
        "print(tuples)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PNmsqeWYBrIf",
        "outputId": "604e6380-e346-4b9e-94e1-8945ca4b4eca"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('L', 1, 'D', 'd', [('d', [-1, -1, 1, -1, -1, -1, -1, -1, 1, -1, -1, 1, 1, -1, -1, -1, -1, -1, -1, -1, 0, -1, 0, 0])]), ('L', 0, 'ü', 'y', [('y', [1, 1, -1, 1, -1, -1, -1, -1, 1, -1, -1, 0, -1, 0, 1, 1, -1, -1, 1, -1, 1, -1, 0, 0])]), ('L', 0, 'ğ', 'ɰ', [('ɰ', [-1, 1, -1, 1, 0, -1, -1, -1, 1, -1, -1, -1, -1, 0, -1, 1, -1, 1, -1, -1, 1, -1, 0, 0])]), ('L', 0, 'ü', 'y', [('y', [1, 1, -1, 1, -1, -1, -1, -1, 1, -1, -1, 0, -1, 0, 1, 1, -1, -1, 1, -1, 1, -1, 0, 0])]), ('L', 0, 'n', 'n', [('n', [-1, 1, 1, -1, -1, -1, 1, -1, 1, -1, -1, 1, 1, -1, -1, -1, -1, -1, -1, -1, 0, -1, 0, 0])])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here's a breakdown of the code you provided:\n",
        "\n",
        "```python\n",
        "import epitran\n",
        "\n",
        "# Initialize Epitran for Turkish\n",
        "epi_turkish = epitran.Epitran('tur-Latn')\n",
        "```\n",
        "- **Importing Epitran**: This line imports the Epitran library, which is used for transliterating text from various languages to IPA (International Phonetic Alphabet).\n",
        "- **Initializing Epitran**: This line creates an instance of the `Epitran` class specifically for the Turkish language using the Latin script (`tur-Latn`). This instance will be used for transliterating Turkish text.\n",
        "\n",
        "```python\n",
        "# Get word to tuples\n",
        "tuples = epi_turkish.word_to_tuples(u'Düğün')\n",
        "```\n",
        "- **Getting Word Tuples**: This line calls the `word_to_tuples` method on the `epi_turkish` instance. It takes the Unicode string `u'Düğün'` (which means \"wedding\" in Turkish) as input. The method returns a list of tuples, where each tuple contains detailed information about each phonetic segment of the word.\n",
        "\n",
        "```python\n",
        "print(tuples)\n",
        "```\n",
        "- **Printing the Result**: This line prints the list of tuples returned by `word_to_tuples`. Each tuple provides structured information about each character in the input word, including its category, whether it's uppercase, the orthographic form, the phonetic form, and other phonological features.\n",
        "\n",
        "### Example Output\n",
        "The output will look something like this (the actual output may vary depending on the implementation):\n",
        "```python\n",
        "[\n",
        "    (u'L', 1, u'D', u'd', [...]),\n",
        "    (u'L', 0, u'u\\u0308', u'y', [...]),\n",
        "    (u'L', 0, u'g\\u0306', u'ɰ', [...]),\n",
        "    (u'L', 0, u'u\\u0308', u'y', [...]),\n",
        "    (u'L', 0, u'n', u'n', [...])\n",
        "]\n",
        "```\n",
        "- **Tuple Breakdown**:\n",
        "  - `u'L'`: Character category (L for letter).\n",
        "  - `1`: Indicates if the character is uppercase (1 for yes, 0 for no).\n",
        "  - `u'D'`: The original orthographic form.\n",
        "  - `u'd'`: The phonetic representation.\n",
        "  - `[...]`: A placeholder for additional details, such as phonological features.\n",
        "\n",
        "### Why Use This?\n",
        "Using `word_to_tuples` is beneficial for understanding how each part of a word is pronounced and for applications like speech synthesis, linguistic analysis, or language learning tools. It provides a fine-grained look at the phonetic structure of words in Turkish.\n",
        "\n",
        "In the output of the `word_to_tuples` method, `u'L'` indicates that the character is categorized as a \"letter.\" It does not refer to the presence of the letter \"L\" in the word \"Düğün.\"\n",
        "\n",
        "Here's how to interpret the tuples:\n",
        "\n",
        "- **Character Category** (`u'L'`): This signifies that the character is a letter. It’s part of the Unicode standard's classification system, which categorizes characters into different groups (letters, numbers, punctuation, etc.).\n",
        "- **Orthographic Form**: This is the actual character from the input word.\n",
        "- **Phonetic Form**: This is how that character (or cluster of characters) is represented phonetically in IPA.\n",
        "\n",
        "So, in the tuples for \"Düğün,\" you won't find the letter \"L\" because it's not part of that word. Instead, you'll see other letters like 'D', 'ü', 'ğ', 'ü', and 'n', each with their corresponding phonetic representations. The `u'L'` just indicates that those characters are all classified as letters."
      ],
      "metadata": {
        "id": "vzaetT-7DdDZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install -y flite"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nsMKP3wuFBUP",
        "outputId": "2f761518-8259-407a-d69a-fb088902be30"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  flite\n",
            "0 upgraded, 1 newly installed, 0 to remove and 49 not upgraded.\n",
            "Need to get 254 kB of archives.\n",
            "After this operation, 642 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 flite amd64 2.2-3 [254 kB]\n",
            "Fetched 254 kB in 1s (292 kB/s)\n",
            "Selecting previously unselected package flite.\n",
            "(Reading database ... 123599 files and directories currently installed.)\n",
            "Preparing to unpack .../archives/flite_2.2-3_amd64.deb ...\n",
            "Unpacking flite (2.2-3) ...\n",
            "Setting up flite (2.2-3) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mixed Scripts: The Backoff class is useful when you're dealing with text that may contain multiple languages/scripts. If it can't transliterate a word in the primary language, it falls back to the next one in the list."
      ],
      "metadata": {
        "id": "xW08y1wKFg67"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from epitran.backoff import Backoff\n",
        "\n",
        "# Initialize Backoff for Hindi, English, and Mandarin\n",
        "backoff = Backoff(['hin-Deva', 'eng-Latn', 'cmn-Hans'], cedict_file='/content/sample_data/cedict_1_0_ts_utf-8_mdbg.txt')\n",
        "\n",
        "# Transliterate words in mixed scripts\n",
        "print(backoff.transliterate('हिन्दी'))   # Output: ɦindiː\n",
        "# print(backoff.transliterate('Englishhh'))   # Output: ɪŋɡlɪʃ\n",
        "print(backoff.transliterate('中文'))       # Output: ʈ͡ʂoŋwən\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a3Y5Pt18Dh1E",
        "outputId": "a3b6c930-6ed4-4261-beef-73d82f3f10d8"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ɦindiː\n",
            "ʈ͡ʂoŋwen\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Backoff works on a token-by-token basis: tokens that contain mixed scripts will be returned as the empty string, since they cannot be fully converted by any of the modes.\n",
        "\n",
        "The Backoff class has the following public methods:\n",
        "\n",
        "transliterate: returns a unicode string of IPA phonemes\n",
        "trans_list: returns a list of IPA unicode strings, each of which is a phoneme\n",
        "xsampa_list: returns a list of X-SAMPA (ASCII) strings, each of which is phoneme"
      ],
      "metadata": {
        "id": "yu148NEMGJpO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(backoff.transliterate('हिन्दी'))\n",
        "print(backoff.trans_list('हिन्दी'))\n",
        "\n",
        "print(backoff.xsampa_list('हिन्दी'))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r_jaiSGhFoJE",
        "outputId": "32aadabe-da6a-4949-8f9a-db9d87f4082c"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ɦindiː\n",
            "['ɦ', 'i', 'n', 'd', 'iː']\n",
            "['h\\\\', 'i', 'n', 'd', 'i:']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#whthout preprocess\n",
        "# Initialize Epitran with preprocessors\n",
        "epi_french = epitran.Epitran('fra-Latn')\n",
        "\n",
        "# Transliterate a French word\n",
        "ipa_transcription = epi_french.transliterate(u\"c'est\")\n",
        "print(ipa_transcription)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g3fGcgqcGkAW",
        "outputId": "b2a84788-87c2-4ebc-e333-451a0017b2bc"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "k'ɛs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#with preprocess\n",
        "epi_french = epitran.Epitran('fra-Latn',preproc=True)\n",
        "\n",
        "# Transliterate a French word\n",
        "ipa_transcription = epi_french.transliterate(u\"c'est\")\n",
        "print(ipa_transcription)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d6-xZsQdHIlp",
        "outputId": "2ed2f4c3-14bc-4383-cc77-17137e1e2116"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "k'ɛs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://pypi.org/project/epitran/ --> refer this for epitran"
      ],
      "metadata": {
        "id": "VnUDNiyUId7P"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "li_b5WQLIjr0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}