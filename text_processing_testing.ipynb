{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMIO5EFcj5ihdnbdW1+rV1J",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Abhilitcode/DaveAI_Internship/blob/main/text_processing_testing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k2hHCTRBuid6",
        "outputId": "21d605c1-8ed3-419a-aae8-7ca16c349310"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Arabic Unicode Range: ['\\u0600', '\\u0601', '\\u0602', '\\u0603', '\\u0604', '\\u0605', '؆', '؇', '؈', '؉', '؊', '؋', '،', '؍', '؎', '؏', 'ؐ', 'ؑ', 'ؒ', 'ؓ', 'ؔ', 'ؕ', 'ؖ', 'ؗ', 'ؘ', 'ؙ', 'ؚ', '؛', '\\u061c', '\\u061d', '؞', '؟', 'ؠ', 'ء', 'آ', 'أ', 'ؤ', 'إ', 'ئ', 'ا', 'ب', 'ة', 'ت', 'ث', 'ج', 'ح', 'خ', 'د', 'ذ', 'ر', 'ز', 'س', 'ش', 'ص', 'ض', 'ط', 'ظ', 'ع', 'غ', 'ػ', 'ؼ', 'ؽ', 'ؾ', 'ؿ', 'ـ', 'ف', 'ق', 'ك', 'ل', 'م', 'ن', 'ه', 'و', 'ى', 'ي', 'ً', 'ٌ', 'ٍ', 'َ', 'ُ', 'ِ', 'ّ', 'ْ', 'ٓ', 'ٔ', 'ٕ', 'ٖ', 'ٗ', '٘', 'ٙ', 'ٚ', 'ٛ', 'ٜ', 'ٝ', 'ٞ', 'ٟ', '٠', '١', '٢', '٣', '٤', '٥', '٦', '٧', '٨', '٩', '٪', '٫', '٬', '٭', 'ٮ', 'ٯ', 'ٰ', 'ٱ', 'ٲ', 'ٳ', 'ٴ', 'ٵ', 'ٶ', 'ٷ', 'ٸ', 'ٹ', 'ٺ', 'ٻ', 'ټ', 'ٽ', 'پ', 'ٿ', 'ڀ', 'ځ', 'ڂ', 'ڃ', 'ڄ', 'څ', 'چ', 'ڇ', 'ڈ', 'ډ', 'ڊ', 'ڋ', 'ڌ', 'ڍ', 'ڎ', 'ڏ', 'ڐ', 'ڑ', 'ڒ', 'ړ', 'ڔ', 'ڕ', 'ږ', 'ڗ', 'ژ', 'ڙ', 'ښ', 'ڛ', 'ڜ', 'ڝ', 'ڞ', 'ڟ', 'ڠ', 'ڡ', 'ڢ', 'ڣ', 'ڤ', 'ڥ', 'ڦ', 'ڧ', 'ڨ', 'ک', 'ڪ', 'ګ', 'ڬ', 'ڭ', 'ڮ', 'گ', 'ڰ', 'ڱ', 'ڲ', 'ڳ', 'ڴ', 'ڵ', 'ڶ', 'ڷ', 'ڸ', 'ڹ', 'ں', 'ڻ', 'ڼ', 'ڽ', 'ھ', 'ڿ', 'ۀ', 'ہ', 'ۂ', 'ۃ', 'ۄ', 'ۅ', 'ۆ', 'ۇ', 'ۈ', 'ۉ', 'ۊ', 'ۋ', 'ی', 'ۍ', 'ێ', 'ۏ', 'ې', 'ۑ', 'ے', 'ۓ', '۔', 'ە', 'ۖ', 'ۗ', 'ۘ', 'ۙ', 'ۚ', 'ۛ', 'ۜ', '\\u06dd', '۞', '۟', '۠', 'ۡ', 'ۢ', 'ۣ', 'ۤ', 'ۥ', 'ۦ', 'ۧ', 'ۨ', '۩', '۪', '۫', '۬', 'ۭ', 'ۮ', 'ۯ', '۰', '۱', '۲', '۳', '۴', '۵', '۶', '۷', '۸', '۹', 'ۺ', 'ۻ', 'ۼ', '۽', '۾', 'ۿ']\n",
            "Converted String: This is a test and it should convert to and.\n",
            "Month 1: January\n",
            "Day 21: Twenty First\n"
          ]
        }
      ],
      "source": [
        "import string\n",
        "\n",
        "# Mock PUNCTS and other constants\n",
        "PUNCTS = \"?.,!''\\\"\"\n",
        "ABBREVIATION_DIR = \"/path/to/abbreviation\"  # This can be mocked\n",
        "\n",
        "# 1. Test generate_unicodes\n",
        "def generate_unicodes(start: str, end: str):\n",
        "    def _str_to_hex(val):\n",
        "        return int(hex(int(val, 16)), 16)\n",
        "\n",
        "    start = _str_to_hex(start)\n",
        "    end = _str_to_hex(end)\n",
        "    return [chr(codepoint) for codepoint in range(start, end + 1)]\n",
        "\n",
        "# Test generating unicode range for Arabic (0600 to 06FF)\n",
        "arabic_unicode = generate_unicodes(\"0600\", \"06FF\")\n",
        "print(\"Generated Arabic Unicode Range:\", arabic_unicode)\n",
        "\n",
        "# 2. Test CUSTOM_CONVERSION dictionary\n",
        "CUSTOM_CONVERSION = {\n",
        "    \"&\": \"and\",\n",
        "    \"’\": \"'\",\n",
        "}\n",
        "test_string = \"This is a test & it should convert to and.\"\n",
        "converted_string = test_string\n",
        "for key, value in CUSTOM_CONVERSION.items():\n",
        "    converted_string = converted_string.replace(key, value)\n",
        "\n",
        "print(\"Converted String:\", converted_string)\n",
        "\n",
        "# 3. Test MONTHS and DAYS dictionary\n",
        "MONTHS = {\n",
        "    1: \"January\", 2: \"February\", 3: \"March\", 4: \"April\",\n",
        "    5: \"May\", 6: \"June\", 7: \"July\", 8: \"August\",\n",
        "    9: \"September\", 10: \"October\", 11: \"November\", 12: \"December\"\n",
        "}\n",
        "DAYS = {\n",
        "    1: [\"First\"], 2: [\"Second\"], 3: [\"Third\"], 4: [\"Fourth\"],\n",
        "    5: [\"Fifth\"], 6: [\"Sixth\"], 7: [\"Seventh\"], 8: [\"Eighth\"],\n",
        "    9: [\"Nineth\"], 10: [\"Tenth\"], 11: [\"Eleventh\"], 12: [\"Twelfth\"],\n",
        "    13: [\"Thirteenth\"], 14: [\"Fourteenth\"], 15: [\"Fifteenth\"],\n",
        "    16: [\"Sixteenth\"], 17: [\"Seventh\"], 18: [\"Eighteenth\"],\n",
        "    19: [\"Nineteenth\"], 20: [\"Twentieth\"], 21: [\"Twenty\", \"First\"],\n",
        "    22: [\"Twenty\", \"Second\"], 23: [\"Twenty\", \"Third\"],\n",
        "    24: [\"Twenty\", \"Fourth\"], 25: [\"Twenty\", \"Fifth\"],\n",
        "    26: [\"Twenty\", \"Sixth\"], 27: [\"Twenty\", \"Seventh\"],\n",
        "    28: [\"Twenty\", \"Eighth\"], 29: [\"Twenty\", \"Nineth\"],\n",
        "    30: [\"Thirtieth\"], 31: [\"Thirty\", \"First\"]\n",
        "}\n",
        "\n",
        "print(\"Month 1:\", MONTHS[1])\n",
        "print(\"Day 21:\", ' '.join(DAYS[21]))\n",
        "\n",
        "# If you have access to 'nemo_text_processing', you can load normalizers, but this will require access to the library and its dependencies\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DAYS"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tmmyOKpHE6ip",
        "outputId": "bf3e6a3f-b762-4bfe-d700-93e49b7584fa"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{1: ['First'],\n",
              " 2: ['Second'],\n",
              " 3: ['Third'],\n",
              " 4: ['Fourth'],\n",
              " 5: ['Fifth'],\n",
              " 6: ['Sixth'],\n",
              " 7: ['Seventh'],\n",
              " 8: ['Eighth'],\n",
              " 9: ['Nineth'],\n",
              " 10: ['Tenth'],\n",
              " 11: ['Eleventh'],\n",
              " 12: ['Twelfth'],\n",
              " 13: ['Thirteenth'],\n",
              " 14: ['Fourteenth'],\n",
              " 15: ['Fifteenth'],\n",
              " 16: ['Sixteenth'],\n",
              " 17: ['Seventh'],\n",
              " 18: ['Eighteenth'],\n",
              " 19: ['Nineteenth'],\n",
              " 20: ['Twentieth'],\n",
              " 21: ['Twenty', 'First'],\n",
              " 22: ['Twenty', 'Second'],\n",
              " 23: ['Twenty', 'Third'],\n",
              " 24: ['Twenty', 'Fourth'],\n",
              " 25: ['Twenty', 'Fifth'],\n",
              " 26: ['Twenty', 'Sixth'],\n",
              " 27: ['Twenty', 'Seventh'],\n",
              " 28: ['Twenty', 'Eighth'],\n",
              " 29: ['Twenty', 'Nineth'],\n",
              " 30: ['Thirtieth'],\n",
              " 31: ['Thirty', 'First']}"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MONTHS"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vc5fn-8xE9XJ",
        "outputId": "2033fced-216c-4bf4-fe28-365b501f6462"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{1: 'January',\n",
              " 2: 'February',\n",
              " 3: 'March',\n",
              " 4: 'April',\n",
              " 5: 'May',\n",
              " 6: 'June',\n",
              " 7: 'July',\n",
              " 8: 'August',\n",
              " 9: 'September',\n",
              " 10: 'October',\n",
              " 11: 'November',\n",
              " 12: 'December'}"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TextNormalizer:\n",
        "    def __init__(self, lang: str, utterance: str):\n",
        "        self.lang = lang\n",
        "        self.utterance = utterance\n",
        "\n",
        "    def _pre_normalize(self):\n",
        "        '''\n",
        "        Step carried out immediately text is received to remove or add unrequired spaces.\n",
        "        '''\n",
        "        if self.lang == \"hindi\":\n",
        "            self.utterance = self.utterance.replace('।', '.')\n",
        "            self.utterance = self.utterance.replace('|', '.')\n",
        "\n",
        "        while \"  \" in self.utterance:\n",
        "            self.utterance = self.utterance.replace(\"  \", \" \")\n",
        "\n",
        "        # Simulating punctuation checks\n",
        "        PUNCTS = \"?.,!''\\\"\"\n",
        "        for c in PUNCTS:\n",
        "            if f\" {c}\" in self.utterance:\n",
        "                self.utterance = self.utterance.replace(f\" {c}\", f\"{c}\")\n",
        "\n",
        "# Testing the _pre_normalize method\n",
        "def test_pre_normalize():\n",
        "    test_cases = [\n",
        "        (\"hindi\", \"यह एक टेस्ट । |\", \"Expected output: 'यह एक टेस्ट . .'\"),\n",
        "        (\"hindi\", \"यह   एक    और   टेस्ट ।\", \"Expected output: 'यह एक और टेस्ट . .'\"),\n",
        "        (\"english\", \"This is a test, with  multiple   spaces.\", \"Expected output: 'This is a test, with multiple spaces.'\")\n",
        "    ]\n",
        "\n",
        "    for lang, text, expected in test_cases:\n",
        "        normalizer = TextNormalizer(lang, text)\n",
        "        normalizer._pre_normalize()\n",
        "        print(f\"Input: {text}\")\n",
        "        print(f\"Normalized: {normalizer.utterance}\")\n",
        "        print(f\"Expected: {expected}\")\n",
        "        print(\"-----\")\n",
        "\n",
        "# Run the test\n",
        "test_pre_normalize()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ZO1NI_6FCmP",
        "outputId": "6d0d5631-e16d-47c3-c4f7-fa0f2e6cb22e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: यह एक टेस्ट । |\n",
            "Normalized: यह एक टेस्ट..\n",
            "Expected: Expected output: 'यह एक टेस्ट . .'\n",
            "-----\n",
            "Input: यह   एक    और   टेस्ट ।\n",
            "Normalized: यह एक और टेस्ट.\n",
            "Expected: Expected output: 'यह एक और टेस्ट . .'\n",
            "-----\n",
            "Input: This is a test, with  multiple   spaces.\n",
            "Normalized: This is a test, with multiple spaces.\n",
            "Expected: Expected output: 'This is a test, with multiple spaces.'\n",
            "-----\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# Example of custom conversions (to simulate the behavior of CUSTOM_CONVERSION)\n",
        "CUSTOM_CONVERSION = {\n",
        "    \"&\": \"and\",\n",
        "    \"’\": \"'\",\n",
        "}\n",
        "\n",
        "class TextNormalizer:\n",
        "    def __init__(self, lang: str, utterance: str, cust_pron: dict):\n",
        "        self.lang = lang\n",
        "        self.utterance = utterance\n",
        "        self.cust_pron = cust_pron\n",
        "\n",
        "    def _custom_changes(self):\n",
        "        '''\n",
        "        Function where pronunciation of words is made.\n",
        "        '''\n",
        "        # Apply custom pronunciations from cust_pron\n",
        "        for k, v in self.cust_pron.items():\n",
        "            self.utterance = re.sub(r'\\b' + re.escape(k) + r'\\b', v, self.utterance)\n",
        "\n",
        "        # Apply custom conversions like '&' to 'and', etc.\n",
        "        for k, v in CUSTOM_CONVERSION.items():\n",
        "            self.utterance = self.utterance.replace(k, v)\n",
        "\n",
        "# Testing the _custom_changes method\n",
        "def test_custom_changes():\n",
        "    test_cases = [\n",
        "        (\"english\", \"I & you ’ll make it.\", {\"&\": \"and\", \"’ll\": \"will\"}),\n",
        "        (\"english\", \"It's a test case with & and ’.\", {\"case\": \"test-case\"}),\n",
        "        (\"english\", \"Hello & welcome!\", {}),\n",
        "        (\"english\", \"Let’s go to the park.\", {})\n",
        "    ]\n",
        "\n",
        "    for lang, text, pron_dict in test_cases:\n",
        "        normalizer = TextNormalizer(lang, text, pron_dict)\n",
        "        normalizer._custom_changes()\n",
        "        print(f\"Input: {text}\")\n",
        "        print(f\"Normalized: {normalizer.utterance}\")\n",
        "        print(\"-----\")\n",
        "\n",
        "# Run the test\n",
        "test_custom_changes()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "owd8qEn9HG92",
        "outputId": "9b69cabe-2809-4114-9551-8e567c3a5547"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: I & you ’ll make it.\n",
            "Normalized: I and you 'll make it.\n",
            "-----\n",
            "Input: It's a test case with & and ’.\n",
            "Normalized: It's a test test-case with and and '.\n",
            "-----\n",
            "Input: Hello & welcome!\n",
            "Normalized: Hello and welcome!\n",
            "-----\n",
            "Input: Let’s go to the park.\n",
            "Normalized: Let's go to the park.\n",
            "-----\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the PUNCTS variable as used in the method\n",
        "PUNCTS = \"?.,!\"\n",
        "\n",
        "class TextNormalizer:\n",
        "    def _is_tag(self, word: str):\n",
        "        '''\n",
        "        Checking whether a text is a valid ssml tag.\n",
        "        '''\n",
        "        if word.startswith(\"<\") and word.endswith(\">\"):\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "    def _correct_spacing(self, word: str):\n",
        "        '''\n",
        "        Function to correct spacing issues where punctuations are present.\n",
        "        '''\n",
        "        for p in PUNCTS:\n",
        "            word = word.replace(p, f\"{p} \")\n",
        "            word = word.replace(f\" {p}\", p)\n",
        "        return word.strip()\n",
        "\n",
        "# Instantiate the class\n",
        "normalizer = TextNormalizer()\n",
        "\n",
        "# Testing the _is_tag function\n",
        "test_cases_is_tag = [\n",
        "    \"<speak>\",   # Expected: True\n",
        "    \"text\",      # Expected: False\n",
        "    \"</voice>\",  # Expected: True\n",
        "    \"<tag\",      # Expected: False\n",
        "    \"not>a\"      # Expected: False\n",
        "]\n",
        "\n",
        "print(\"Testing _is_tag function:\")\n",
        "for case in test_cases_is_tag:\n",
        "    print(f\"Input: '{case}' -> Output: {normalizer._is_tag(case)}\")\n",
        "\n",
        "# Testing the _correct_spacing function\n",
        "test_cases_correct_spacing = [\n",
        "    \"Hello ,world !\",  # Expected: \"Hello, world!\"\n",
        "    \"Test!Test\",       # Expected: \"Test! Test\"\n",
        "    \"Well, done.\",     # Expected: \"Well, done.\"\n",
        "    \"Spaces?Everywhere\", # Expected: \"Spaces? Everywhere\"\n",
        "]\n",
        "\n",
        "print(\"\\nTesting _correct_spacing function:\")\n",
        "for case in test_cases_correct_spacing:\n",
        "    print(f\"Input: '{case}' -> Output: '{normalizer._correct_spacing(case)}'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dvXC6hzJT2vb",
        "outputId": "87359ce9-a433-49a1-c88a-b3ba7e5405a5"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing _is_tag function:\n",
            "Input: '<speak>' -> Output: True\n",
            "Input: 'text' -> Output: False\n",
            "Input: '</voice>' -> Output: True\n",
            "Input: '<tag' -> Output: False\n",
            "Input: 'not>a' -> Output: False\n",
            "\n",
            "Testing _correct_spacing function:\n",
            "Input: 'Hello ,world !' -> Output: 'Hello, world!'\n",
            "Input: 'Test!Test' -> Output: 'Test! Test'\n",
            "Input: 'Well, done.' -> Output: 'Well,  done.'\n",
            "Input: 'Spaces?Everywhere' -> Output: 'Spaces? Everywhere'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import string\n",
        "\n",
        "class TextNormalizer:\n",
        "    def __init__(self, utterance):\n",
        "        self.utterance = utterance\n",
        "\n",
        "    def _is_tag(self, word: str):\n",
        "        '''\n",
        "        Checking whether a text is a valid ssml tag.\n",
        "        '''\n",
        "        return word.startswith(\"<\") and word.endswith(\">\")\n",
        "\n",
        "    def _normalize_date(self, word: str):\n",
        "        '''\n",
        "        Mock function for date normalization. In practice, replace with actual normalization logic.\n",
        "        '''\n",
        "        # Just a placeholder, returning the word itself for testing.\n",
        "        return [word]\n",
        "\n",
        "    def _split_ssml_tags(self):\n",
        "        '''\n",
        "        Received utterance contains both text and ssml tags.\n",
        "        The ssml tags need to be separated for utterance use cases that do not need ssml tags.\n",
        "        '''\n",
        "        tags_pattern = re.compile(r'(<.*?>)')\n",
        "        split_result = re.split(tags_pattern, self.utterance)\n",
        "        result_list = []\n",
        "\n",
        "        for spl in split_result:\n",
        "            if not spl.strip():\n",
        "                continue\n",
        "\n",
        "            if self._is_tag(spl):\n",
        "                result_list.append(spl)\n",
        "            else:\n",
        "                for word in spl.strip().split(' '):\n",
        "                    if word[-1] in string.punctuation:\n",
        "                        temp_w = word[:-1]\n",
        "                        punct = word[-1]\n",
        "                    else:\n",
        "                        punct = \"\"\n",
        "                        temp_w = word\n",
        "                    word = temp_w + punct\n",
        "                    if word:\n",
        "                        result_list.extend(self._normalize_date(word))\n",
        "\n",
        "        return result_list\n",
        "\n",
        "# Example usage\n",
        "utterance = \"Hello <speak> this is a <break time='1s'/> test.\"\n",
        "normalizer = TextNormalizer(utterance)\n",
        "result = normalizer._split_ssml_tags()\n",
        "\n",
        "print(\"Result of _split_ssml_tags:\")\n",
        "print(result)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UVwcp8jRT2sk",
        "outputId": "eb2eabca-057c-4ef5-96fa-6c5075f53fd0"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result of _split_ssml_tags:\n",
            "['Hello', '<speak>', 'this', 'is', 'a', \"<break time='1s'/>\", 'test.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import string\n",
        "\n",
        "# Define constants for testing\n",
        "PUNCTS = string.punctuation\n",
        "EMAIL_SYNTAX = r\"(^[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+$)\"\n",
        "ALLOWED_CHARACTERS = \"ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789\"\n",
        "EXCEMPTED_CHARACTERS = \"'\"\n",
        "\n",
        "class TextNormalizer:\n",
        "    def __init__(self, normalizer=None):\n",
        "        self.normalizer = normalizer\n",
        "\n",
        "    def _is_tag(self, word: str):\n",
        "        return word.startswith(\"<\") and word.endswith(\">\")\n",
        "\n",
        "    def _correct_spacing(self, word: str):\n",
        "        for p in PUNCTS:\n",
        "            word = word.replace(p, f\"{p} \")\n",
        "            word = word.replace(f\" {p}\", p)\n",
        "        return word.strip()\n",
        "\n",
        "    def _is_email(self, utterance_list: list):\n",
        "        utt_list = []\n",
        "        for word in utterance_list:\n",
        "            b = \"\"\n",
        "            w = word\n",
        "            if word[0] in PUNCTS:\n",
        "                b = w[0]\n",
        "                w = w[1:]\n",
        "            a = \"\"\n",
        "            if word[-1] in PUNCTS:\n",
        "                a = w[-1]\n",
        "                w = w[:-1]\n",
        "            if re.fullmatch(EMAIL_SYNTAX, w):\n",
        "                if self.normalizer:\n",
        "                    w = self.normalizer.normalize(w)\n",
        "            elif self._is_tag(w):\n",
        "                w = self._correct_spacing(w)\n",
        "            utt_list.append(b + w + a)\n",
        "        return utt_list\n",
        "\n",
        "    def _remove_successive_puncts(self, utterance_list: list):\n",
        "        lst = []\n",
        "        for utt in utterance_list:\n",
        "            utt = utt.strip()\n",
        "            if self._is_tag(utt):\n",
        "                n = utt\n",
        "            else:\n",
        "                n = \"\"\n",
        "                for i, c in enumerate(utt):\n",
        "                    if i > 0:\n",
        "                        if (n[-1] == \" \" and c == \" \") or (n[-1] in string.punctuation and c in string.punctuation):\n",
        "                            continue\n",
        "                    n += c\n",
        "            lst.append(n)\n",
        "        return lst\n",
        "\n",
        "    def _remove_emojis(self, inp: str):\n",
        "        emoji_pattern = re.compile(\"[\"\n",
        "            u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "            u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "            u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "            u\"\\U0001F700-\\U0001F77F\"  # alchemical symbols\n",
        "            u\"\\U0001F780-\\U0001F7FF\"  # Geometric Shapes Extended\n",
        "            u\"\\U0001F800-\\U0001F8FF\"  # Supplemental Arrows-C\n",
        "            u\"\\U0001F900-\\U0001F9FF\"  # Supplemental Symbols and Pictographs\n",
        "            u\"\\U0001FA00-\\U0001FA6F\"  # Chess Symbols\n",
        "            u\"\\U0001FA70-\\U0001FAFF\"  # Symbols and Pictographs Extended-A\n",
        "            u\"\\U00002702-\\U000027B0\"  # Dingbats\n",
        "            u\"\\U000024C2-\\U0001F251\"  # Enclosed characters\n",
        "            \"]+\", flags=re.UNICODE)\n",
        "        return emoji_pattern.sub(r'', inp)\n",
        "\n",
        "    def remove_not_allowed_characters(self, utt):\n",
        "        new_utt = \"\"\n",
        "        for c in utt:\n",
        "            if c in ALLOWED_CHARACTERS:\n",
        "                new_utt += c\n",
        "            elif c.isalpha() or c.isdigit() or not c.strip() or c in PUNCTS or c in EXCEMPTED_CHARACTERS:\n",
        "                if len(new_utt) > 1 and c in PUNCTS and not new_utt[-1].strip():\n",
        "                    new_utt = new_utt[:-1]\n",
        "                new_utt += c\n",
        "            else:\n",
        "                new_utt += \" \"\n",
        "        return new_utt\n",
        "\n",
        "    def post_normalization(self, utt: str, ssml: bool):\n",
        "        if not ssml:\n",
        "            utt = self._remove_successive_puncts([utt])[0]\n",
        "        utt = self._correct_spacing(utt)\n",
        "        while \"  \" in utt:\n",
        "            utt = utt.replace(\"  \", \" \")\n",
        "        return utt\n",
        "\n",
        "# Testing the functions\n",
        "normalizer = TextNormalizer()\n",
        "\n",
        "# Example test for `_is_email`\n",
        "test_utterance_list = [\"hello@example.com\", \"test@sample.com,\", \"<test>\"]\n",
        "print(\"_is_email test result:\", normalizer._is_email(test_utterance_list))\n",
        "\n",
        "# Example test for `_remove_successive_puncts`\n",
        "test_punct_list = [\"Hello,, there!!!\", \"Hi!!\", \"<tag>\"]\n",
        "print(\"_remove_successive_puncts test result:\", normalizer._remove_successive_puncts(test_punct_list))\n",
        "\n",
        "# Example test for `_remove_emojis`\n",
        "test_emoji_input = \"Hello 😊, world! 🌍\"\n",
        "print(\"_remove_emojis test result:\", normalizer._remove_emojis(test_emoji_input))\n",
        "\n",
        "# Example test for `remove_not_allowed_characters`\n",
        "test_remove_chars_input = \"Hello@# World! This is a test.\"\n",
        "print(\"remove_not_allowed_characters test result:\", normalizer.remove_not_allowed_characters(test_remove_chars_input))\n",
        "\n",
        "# Example test for `post_normalization`\n",
        "test_post_norm_input = \"This  is  a  test!!\"\n",
        "print(\"post_normalization test result:\", normalizer.post_normalization(test_post_norm_input, ssml=False))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gu3gXYC7T2pa",
        "outputId": "768bfaa8-abcd-4ad9-ac0d-256cc9f7599d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "_is_email test result: ['hello@example.com', 'test@sample.com,', '<test>']\n",
            "_remove_successive_puncts test result: ['Hello, there!', 'Hi!', '<tag>']\n",
            "_remove_emojis test result: Hello , world! \n",
            "remove_not_allowed_characters test result: Hello@# World! This is a test.\n",
            "post_normalization test result: This is a test!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8rInn8Q4T2ml"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dOMRmyvBT2jR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}