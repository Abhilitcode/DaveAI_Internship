{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMIO5EFcj5ihdnbdW1+rV1J",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Abhilitcode/DaveAI_Internship/blob/main/text_processing_testing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k2hHCTRBuid6",
        "outputId": "21d605c1-8ed3-419a-aae8-7ca16c349310"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Arabic Unicode Range: ['\\u0600', '\\u0601', '\\u0602', '\\u0603', '\\u0604', '\\u0605', 'ÿÜ', 'ÿá', 'ÿà', 'ÿâ', 'ÿä', 'ÿã', 'ÿå', 'ÿç', 'ÿé', 'ÿè', 'ÿê', 'ÿë', 'ÿí', 'ÿì', 'ÿî', 'ÿï', 'ÿñ', 'ÿó', 'ÿò', 'ÿô', 'ÿö', 'ÿõ', '\\u061c', '\\u061d', 'ÿû', 'ÿü', 'ÿ†', 'ÿ°', 'ÿ¢', 'ÿ£', 'ÿ§', 'ÿ•', 'ÿ¶', 'ÿß', 'ÿ®', 'ÿ©', 'ÿ™', 'ÿ´', 'ÿ¨', 'ÿ≠', 'ÿÆ', 'ÿØ', 'ÿ∞', 'ÿ±', 'ÿ≤', 'ÿ≥', 'ÿ¥', 'ÿµ', 'ÿ∂', 'ÿ∑', 'ÿ∏', 'ÿπ', 'ÿ∫', 'ÿª', 'ÿº', 'ÿΩ', 'ÿæ', 'ÿø', 'ŸÄ', 'ŸÅ', 'ŸÇ', 'ŸÉ', 'ŸÑ', 'ŸÖ', 'ŸÜ', 'Ÿá', 'Ÿà', 'Ÿâ', 'Ÿä', 'Ÿã', 'Ÿå', 'Ÿç', 'Ÿé', 'Ÿè', 'Ÿê', 'Ÿë', 'Ÿí', 'Ÿì', 'Ÿî', 'Ÿï', 'Ÿñ', 'Ÿó', 'Ÿò', 'Ÿô', 'Ÿö', 'Ÿõ', 'Ÿú', 'Ÿù', 'Ÿû', 'Ÿü', 'Ÿ†', 'Ÿ°', 'Ÿ¢', 'Ÿ£', 'Ÿ§', 'Ÿ•', 'Ÿ¶', 'Ÿß', 'Ÿ®', 'Ÿ©', 'Ÿ™', 'Ÿ´', 'Ÿ¨', 'Ÿ≠', 'ŸÆ', 'ŸØ', 'Ÿ∞', 'Ÿ±', 'Ÿ≤', 'Ÿ≥', 'Ÿ¥', 'Ÿµ', 'Ÿ∂', 'Ÿ∑', 'Ÿ∏', 'Ÿπ', 'Ÿ∫', 'Ÿª', 'Ÿº', 'ŸΩ', 'Ÿæ', 'Ÿø', '⁄Ä', '⁄Å', '⁄Ç', '⁄É', '⁄Ñ', '⁄Ö', '⁄Ü', '⁄á', '⁄à', '⁄â', '⁄ä', '⁄ã', '⁄å', '⁄ç', '⁄é', '⁄è', '⁄ê', '⁄ë', '⁄í', '⁄ì', '⁄î', '⁄ï', '⁄ñ', '⁄ó', '⁄ò', '⁄ô', '⁄ö', '⁄õ', '⁄ú', '⁄ù', '⁄û', '⁄ü', '⁄†', '⁄°', '⁄¢', '⁄£', '⁄§', '⁄•', '⁄¶', '⁄ß', '⁄®', '⁄©', '⁄™', '⁄´', '⁄¨', '⁄≠', '⁄Æ', '⁄Ø', '⁄∞', '⁄±', '⁄≤', '⁄≥', '⁄¥', '⁄µ', '⁄∂', '⁄∑', '⁄∏', '⁄π', '⁄∫', '⁄ª', '⁄º', '⁄Ω', '⁄æ', '⁄ø', '€Ä', '€Å', '€Ç', '€É', '€Ñ', '€Ö', '€Ü', '€á', '€à', '€â', '€ä', '€ã', '€å', '€ç', '€é', '€è', '€ê', '€ë', '€í', '€ì', '€î', '€ï', '€ñ', '€ó', '€ò', '€ô', '€ö', '€õ', '€ú', '\\u06dd', '€û', '€ü', '€†', '€°', '€¢', '€£', '€§', '€•', '€¶', '€ß', '€®', '€©', '€™', '€´', '€¨', '€≠', '€Æ', '€Ø', '€∞', '€±', '€≤', '€≥', '€¥', '€µ', '€∂', '€∑', '€∏', '€π', '€∫', '€ª', '€º', '€Ω', '€æ', '€ø']\n",
            "Converted String: This is a test and it should convert to and.\n",
            "Month 1: January\n",
            "Day 21: Twenty First\n"
          ]
        }
      ],
      "source": [
        "import string\n",
        "\n",
        "# Mock PUNCTS and other constants\n",
        "PUNCTS = \"?.,!''\\\"\"\n",
        "ABBREVIATION_DIR = \"/path/to/abbreviation\"  # This can be mocked\n",
        "\n",
        "# 1. Test generate_unicodes\n",
        "def generate_unicodes(start: str, end: str):\n",
        "    def _str_to_hex(val):\n",
        "        return int(hex(int(val, 16)), 16)\n",
        "\n",
        "    start = _str_to_hex(start)\n",
        "    end = _str_to_hex(end)\n",
        "    return [chr(codepoint) for codepoint in range(start, end + 1)]\n",
        "\n",
        "# Test generating unicode range for Arabic (0600 to 06FF)\n",
        "arabic_unicode = generate_unicodes(\"0600\", \"06FF\")\n",
        "print(\"Generated Arabic Unicode Range:\", arabic_unicode)\n",
        "\n",
        "# 2. Test CUSTOM_CONVERSION dictionary\n",
        "CUSTOM_CONVERSION = {\n",
        "    \"&\": \"and\",\n",
        "    \"‚Äô\": \"'\",\n",
        "}\n",
        "test_string = \"This is a test & it should convert to and.\"\n",
        "converted_string = test_string\n",
        "for key, value in CUSTOM_CONVERSION.items():\n",
        "    converted_string = converted_string.replace(key, value)\n",
        "\n",
        "print(\"Converted String:\", converted_string)\n",
        "\n",
        "# 3. Test MONTHS and DAYS dictionary\n",
        "MONTHS = {\n",
        "    1: \"January\", 2: \"February\", 3: \"March\", 4: \"April\",\n",
        "    5: \"May\", 6: \"June\", 7: \"July\", 8: \"August\",\n",
        "    9: \"September\", 10: \"October\", 11: \"November\", 12: \"December\"\n",
        "}\n",
        "DAYS = {\n",
        "    1: [\"First\"], 2: [\"Second\"], 3: [\"Third\"], 4: [\"Fourth\"],\n",
        "    5: [\"Fifth\"], 6: [\"Sixth\"], 7: [\"Seventh\"], 8: [\"Eighth\"],\n",
        "    9: [\"Nineth\"], 10: [\"Tenth\"], 11: [\"Eleventh\"], 12: [\"Twelfth\"],\n",
        "    13: [\"Thirteenth\"], 14: [\"Fourteenth\"], 15: [\"Fifteenth\"],\n",
        "    16: [\"Sixteenth\"], 17: [\"Seventh\"], 18: [\"Eighteenth\"],\n",
        "    19: [\"Nineteenth\"], 20: [\"Twentieth\"], 21: [\"Twenty\", \"First\"],\n",
        "    22: [\"Twenty\", \"Second\"], 23: [\"Twenty\", \"Third\"],\n",
        "    24: [\"Twenty\", \"Fourth\"], 25: [\"Twenty\", \"Fifth\"],\n",
        "    26: [\"Twenty\", \"Sixth\"], 27: [\"Twenty\", \"Seventh\"],\n",
        "    28: [\"Twenty\", \"Eighth\"], 29: [\"Twenty\", \"Nineth\"],\n",
        "    30: [\"Thirtieth\"], 31: [\"Thirty\", \"First\"]\n",
        "}\n",
        "\n",
        "print(\"Month 1:\", MONTHS[1])\n",
        "print(\"Day 21:\", ' '.join(DAYS[21]))\n",
        "\n",
        "# If you have access to 'nemo_text_processing', you can load normalizers, but this will require access to the library and its dependencies\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DAYS"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tmmyOKpHE6ip",
        "outputId": "bf3e6a3f-b762-4bfe-d700-93e49b7584fa"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{1: ['First'],\n",
              " 2: ['Second'],\n",
              " 3: ['Third'],\n",
              " 4: ['Fourth'],\n",
              " 5: ['Fifth'],\n",
              " 6: ['Sixth'],\n",
              " 7: ['Seventh'],\n",
              " 8: ['Eighth'],\n",
              " 9: ['Nineth'],\n",
              " 10: ['Tenth'],\n",
              " 11: ['Eleventh'],\n",
              " 12: ['Twelfth'],\n",
              " 13: ['Thirteenth'],\n",
              " 14: ['Fourteenth'],\n",
              " 15: ['Fifteenth'],\n",
              " 16: ['Sixteenth'],\n",
              " 17: ['Seventh'],\n",
              " 18: ['Eighteenth'],\n",
              " 19: ['Nineteenth'],\n",
              " 20: ['Twentieth'],\n",
              " 21: ['Twenty', 'First'],\n",
              " 22: ['Twenty', 'Second'],\n",
              " 23: ['Twenty', 'Third'],\n",
              " 24: ['Twenty', 'Fourth'],\n",
              " 25: ['Twenty', 'Fifth'],\n",
              " 26: ['Twenty', 'Sixth'],\n",
              " 27: ['Twenty', 'Seventh'],\n",
              " 28: ['Twenty', 'Eighth'],\n",
              " 29: ['Twenty', 'Nineth'],\n",
              " 30: ['Thirtieth'],\n",
              " 31: ['Thirty', 'First']}"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MONTHS"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vc5fn-8xE9XJ",
        "outputId": "2033fced-216c-4bf4-fe28-365b501f6462"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{1: 'January',\n",
              " 2: 'February',\n",
              " 3: 'March',\n",
              " 4: 'April',\n",
              " 5: 'May',\n",
              " 6: 'June',\n",
              " 7: 'July',\n",
              " 8: 'August',\n",
              " 9: 'September',\n",
              " 10: 'October',\n",
              " 11: 'November',\n",
              " 12: 'December'}"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TextNormalizer:\n",
        "    def __init__(self, lang: str, utterance: str):\n",
        "        self.lang = lang\n",
        "        self.utterance = utterance\n",
        "\n",
        "    def _pre_normalize(self):\n",
        "        '''\n",
        "        Step carried out immediately text is received to remove or add unrequired spaces.\n",
        "        '''\n",
        "        if self.lang == \"hindi\":\n",
        "            self.utterance = self.utterance.replace('‡•§', '.')\n",
        "            self.utterance = self.utterance.replace('|', '.')\n",
        "\n",
        "        while \"  \" in self.utterance:\n",
        "            self.utterance = self.utterance.replace(\"  \", \" \")\n",
        "\n",
        "        # Simulating punctuation checks\n",
        "        PUNCTS = \"?.,!''\\\"\"\n",
        "        for c in PUNCTS:\n",
        "            if f\" {c}\" in self.utterance:\n",
        "                self.utterance = self.utterance.replace(f\" {c}\", f\"{c}\")\n",
        "\n",
        "# Testing the _pre_normalize method\n",
        "def test_pre_normalize():\n",
        "    test_cases = [\n",
        "        (\"hindi\", \"‡§Ø‡§π ‡§è‡§ï ‡§ü‡•á‡§∏‡•ç‡§ü ‡•§ |\", \"Expected output: '‡§Ø‡§π ‡§è‡§ï ‡§ü‡•á‡§∏‡•ç‡§ü . .'\"),\n",
        "        (\"hindi\", \"‡§Ø‡§π   ‡§è‡§ï    ‡§î‡§∞   ‡§ü‡•á‡§∏‡•ç‡§ü ‡•§\", \"Expected output: '‡§Ø‡§π ‡§è‡§ï ‡§î‡§∞ ‡§ü‡•á‡§∏‡•ç‡§ü . .'\"),\n",
        "        (\"english\", \"This is a test, with  multiple   spaces.\", \"Expected output: 'This is a test, with multiple spaces.'\")\n",
        "    ]\n",
        "\n",
        "    for lang, text, expected in test_cases:\n",
        "        normalizer = TextNormalizer(lang, text)\n",
        "        normalizer._pre_normalize()\n",
        "        print(f\"Input: {text}\")\n",
        "        print(f\"Normalized: {normalizer.utterance}\")\n",
        "        print(f\"Expected: {expected}\")\n",
        "        print(\"-----\")\n",
        "\n",
        "# Run the test\n",
        "test_pre_normalize()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ZO1NI_6FCmP",
        "outputId": "6d0d5631-e16d-47c3-c4f7-fa0f2e6cb22e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: ‡§Ø‡§π ‡§è‡§ï ‡§ü‡•á‡§∏‡•ç‡§ü ‡•§ |\n",
            "Normalized: ‡§Ø‡§π ‡§è‡§ï ‡§ü‡•á‡§∏‡•ç‡§ü..\n",
            "Expected: Expected output: '‡§Ø‡§π ‡§è‡§ï ‡§ü‡•á‡§∏‡•ç‡§ü . .'\n",
            "-----\n",
            "Input: ‡§Ø‡§π   ‡§è‡§ï    ‡§î‡§∞   ‡§ü‡•á‡§∏‡•ç‡§ü ‡•§\n",
            "Normalized: ‡§Ø‡§π ‡§è‡§ï ‡§î‡§∞ ‡§ü‡•á‡§∏‡•ç‡§ü.\n",
            "Expected: Expected output: '‡§Ø‡§π ‡§è‡§ï ‡§î‡§∞ ‡§ü‡•á‡§∏‡•ç‡§ü . .'\n",
            "-----\n",
            "Input: This is a test, with  multiple   spaces.\n",
            "Normalized: This is a test, with multiple spaces.\n",
            "Expected: Expected output: 'This is a test, with multiple spaces.'\n",
            "-----\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# Example of custom conversions (to simulate the behavior of CUSTOM_CONVERSION)\n",
        "CUSTOM_CONVERSION = {\n",
        "    \"&\": \"and\",\n",
        "    \"‚Äô\": \"'\",\n",
        "}\n",
        "\n",
        "class TextNormalizer:\n",
        "    def __init__(self, lang: str, utterance: str, cust_pron: dict):\n",
        "        self.lang = lang\n",
        "        self.utterance = utterance\n",
        "        self.cust_pron = cust_pron\n",
        "\n",
        "    def _custom_changes(self):\n",
        "        '''\n",
        "        Function where pronunciation of words is made.\n",
        "        '''\n",
        "        # Apply custom pronunciations from cust_pron\n",
        "        for k, v in self.cust_pron.items():\n",
        "            self.utterance = re.sub(r'\\b' + re.escape(k) + r'\\b', v, self.utterance)\n",
        "\n",
        "        # Apply custom conversions like '&' to 'and', etc.\n",
        "        for k, v in CUSTOM_CONVERSION.items():\n",
        "            self.utterance = self.utterance.replace(k, v)\n",
        "\n",
        "# Testing the _custom_changes method\n",
        "def test_custom_changes():\n",
        "    test_cases = [\n",
        "        (\"english\", \"I & you ‚Äôll make it.\", {\"&\": \"and\", \"‚Äôll\": \"will\"}),\n",
        "        (\"english\", \"It's a test case with & and ‚Äô.\", {\"case\": \"test-case\"}),\n",
        "        (\"english\", \"Hello & welcome!\", {}),\n",
        "        (\"english\", \"Let‚Äôs go to the park.\", {})\n",
        "    ]\n",
        "\n",
        "    for lang, text, pron_dict in test_cases:\n",
        "        normalizer = TextNormalizer(lang, text, pron_dict)\n",
        "        normalizer._custom_changes()\n",
        "        print(f\"Input: {text}\")\n",
        "        print(f\"Normalized: {normalizer.utterance}\")\n",
        "        print(\"-----\")\n",
        "\n",
        "# Run the test\n",
        "test_custom_changes()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "owd8qEn9HG92",
        "outputId": "9b69cabe-2809-4114-9551-8e567c3a5547"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: I & you ‚Äôll make it.\n",
            "Normalized: I and you 'll make it.\n",
            "-----\n",
            "Input: It's a test case with & and ‚Äô.\n",
            "Normalized: It's a test test-case with and and '.\n",
            "-----\n",
            "Input: Hello & welcome!\n",
            "Normalized: Hello and welcome!\n",
            "-----\n",
            "Input: Let‚Äôs go to the park.\n",
            "Normalized: Let's go to the park.\n",
            "-----\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the PUNCTS variable as used in the method\n",
        "PUNCTS = \"?.,!\"\n",
        "\n",
        "class TextNormalizer:\n",
        "    def _is_tag(self, word: str):\n",
        "        '''\n",
        "        Checking whether a text is a valid ssml tag.\n",
        "        '''\n",
        "        if word.startswith(\"<\") and word.endswith(\">\"):\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "    def _correct_spacing(self, word: str):\n",
        "        '''\n",
        "        Function to correct spacing issues where punctuations are present.\n",
        "        '''\n",
        "        for p in PUNCTS:\n",
        "            word = word.replace(p, f\"{p} \")\n",
        "            word = word.replace(f\" {p}\", p)\n",
        "        return word.strip()\n",
        "\n",
        "# Instantiate the class\n",
        "normalizer = TextNormalizer()\n",
        "\n",
        "# Testing the _is_tag function\n",
        "test_cases_is_tag = [\n",
        "    \"<speak>\",   # Expected: True\n",
        "    \"text\",      # Expected: False\n",
        "    \"</voice>\",  # Expected: True\n",
        "    \"<tag\",      # Expected: False\n",
        "    \"not>a\"      # Expected: False\n",
        "]\n",
        "\n",
        "print(\"Testing _is_tag function:\")\n",
        "for case in test_cases_is_tag:\n",
        "    print(f\"Input: '{case}' -> Output: {normalizer._is_tag(case)}\")\n",
        "\n",
        "# Testing the _correct_spacing function\n",
        "test_cases_correct_spacing = [\n",
        "    \"Hello ,world !\",  # Expected: \"Hello, world!\"\n",
        "    \"Test!Test\",       # Expected: \"Test! Test\"\n",
        "    \"Well, done.\",     # Expected: \"Well, done.\"\n",
        "    \"Spaces?Everywhere\", # Expected: \"Spaces? Everywhere\"\n",
        "]\n",
        "\n",
        "print(\"\\nTesting _correct_spacing function:\")\n",
        "for case in test_cases_correct_spacing:\n",
        "    print(f\"Input: '{case}' -> Output: '{normalizer._correct_spacing(case)}'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dvXC6hzJT2vb",
        "outputId": "87359ce9-a433-49a1-c88a-b3ba7e5405a5"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing _is_tag function:\n",
            "Input: '<speak>' -> Output: True\n",
            "Input: 'text' -> Output: False\n",
            "Input: '</voice>' -> Output: True\n",
            "Input: '<tag' -> Output: False\n",
            "Input: 'not>a' -> Output: False\n",
            "\n",
            "Testing _correct_spacing function:\n",
            "Input: 'Hello ,world !' -> Output: 'Hello, world!'\n",
            "Input: 'Test!Test' -> Output: 'Test! Test'\n",
            "Input: 'Well, done.' -> Output: 'Well,  done.'\n",
            "Input: 'Spaces?Everywhere' -> Output: 'Spaces? Everywhere'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import string\n",
        "\n",
        "class TextNormalizer:\n",
        "    def __init__(self, utterance):\n",
        "        self.utterance = utterance\n",
        "\n",
        "    def _is_tag(self, word: str):\n",
        "        '''\n",
        "        Checking whether a text is a valid ssml tag.\n",
        "        '''\n",
        "        return word.startswith(\"<\") and word.endswith(\">\")\n",
        "\n",
        "    def _normalize_date(self, word: str):\n",
        "        '''\n",
        "        Mock function for date normalization. In practice, replace with actual normalization logic.\n",
        "        '''\n",
        "        # Just a placeholder, returning the word itself for testing.\n",
        "        return [word]\n",
        "\n",
        "    def _split_ssml_tags(self):\n",
        "        '''\n",
        "        Received utterance contains both text and ssml tags.\n",
        "        The ssml tags need to be separated for utterance use cases that do not need ssml tags.\n",
        "        '''\n",
        "        tags_pattern = re.compile(r'(<.*?>)')\n",
        "        split_result = re.split(tags_pattern, self.utterance)\n",
        "        result_list = []\n",
        "\n",
        "        for spl in split_result:\n",
        "            if not spl.strip():\n",
        "                continue\n",
        "\n",
        "            if self._is_tag(spl):\n",
        "                result_list.append(spl)\n",
        "            else:\n",
        "                for word in spl.strip().split(' '):\n",
        "                    if word[-1] in string.punctuation:\n",
        "                        temp_w = word[:-1]\n",
        "                        punct = word[-1]\n",
        "                    else:\n",
        "                        punct = \"\"\n",
        "                        temp_w = word\n",
        "                    word = temp_w + punct\n",
        "                    if word:\n",
        "                        result_list.extend(self._normalize_date(word))\n",
        "\n",
        "        return result_list\n",
        "\n",
        "# Example usage\n",
        "utterance = \"Hello <speak> this is a <break time='1s'/> test.\"\n",
        "normalizer = TextNormalizer(utterance)\n",
        "result = normalizer._split_ssml_tags()\n",
        "\n",
        "print(\"Result of _split_ssml_tags:\")\n",
        "print(result)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UVwcp8jRT2sk",
        "outputId": "eb2eabca-057c-4ef5-96fa-6c5075f53fd0"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result of _split_ssml_tags:\n",
            "['Hello', '<speak>', 'this', 'is', 'a', \"<break time='1s'/>\", 'test.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import string\n",
        "\n",
        "# Define constants for testing\n",
        "PUNCTS = string.punctuation\n",
        "EMAIL_SYNTAX = r\"(^[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+$)\"\n",
        "ALLOWED_CHARACTERS = \"ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789\"\n",
        "EXCEMPTED_CHARACTERS = \"'\"\n",
        "\n",
        "class TextNormalizer:\n",
        "    def __init__(self, normalizer=None):\n",
        "        self.normalizer = normalizer\n",
        "\n",
        "    def _is_tag(self, word: str):\n",
        "        return word.startswith(\"<\") and word.endswith(\">\")\n",
        "\n",
        "    def _correct_spacing(self, word: str):\n",
        "        for p in PUNCTS:\n",
        "            word = word.replace(p, f\"{p} \")\n",
        "            word = word.replace(f\" {p}\", p)\n",
        "        return word.strip()\n",
        "\n",
        "    def _is_email(self, utterance_list: list):\n",
        "        utt_list = []\n",
        "        for word in utterance_list:\n",
        "            b = \"\"\n",
        "            w = word\n",
        "            if word[0] in PUNCTS:\n",
        "                b = w[0]\n",
        "                w = w[1:]\n",
        "            a = \"\"\n",
        "            if word[-1] in PUNCTS:\n",
        "                a = w[-1]\n",
        "                w = w[:-1]\n",
        "            if re.fullmatch(EMAIL_SYNTAX, w):\n",
        "                if self.normalizer:\n",
        "                    w = self.normalizer.normalize(w)\n",
        "            elif self._is_tag(w):\n",
        "                w = self._correct_spacing(w)\n",
        "            utt_list.append(b + w + a)\n",
        "        return utt_list\n",
        "\n",
        "    def _remove_successive_puncts(self, utterance_list: list):\n",
        "        lst = []\n",
        "        for utt in utterance_list:\n",
        "            utt = utt.strip()\n",
        "            if self._is_tag(utt):\n",
        "                n = utt\n",
        "            else:\n",
        "                n = \"\"\n",
        "                for i, c in enumerate(utt):\n",
        "                    if i > 0:\n",
        "                        if (n[-1] == \" \" and c == \" \") or (n[-1] in string.punctuation and c in string.punctuation):\n",
        "                            continue\n",
        "                    n += c\n",
        "            lst.append(n)\n",
        "        return lst\n",
        "\n",
        "    def _remove_emojis(self, inp: str):\n",
        "        emoji_pattern = re.compile(\"[\"\n",
        "            u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "            u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "            u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "            u\"\\U0001F700-\\U0001F77F\"  # alchemical symbols\n",
        "            u\"\\U0001F780-\\U0001F7FF\"  # Geometric Shapes Extended\n",
        "            u\"\\U0001F800-\\U0001F8FF\"  # Supplemental Arrows-C\n",
        "            u\"\\U0001F900-\\U0001F9FF\"  # Supplemental Symbols and Pictographs\n",
        "            u\"\\U0001FA00-\\U0001FA6F\"  # Chess Symbols\n",
        "            u\"\\U0001FA70-\\U0001FAFF\"  # Symbols and Pictographs Extended-A\n",
        "            u\"\\U00002702-\\U000027B0\"  # Dingbats\n",
        "            u\"\\U000024C2-\\U0001F251\"  # Enclosed characters\n",
        "            \"]+\", flags=re.UNICODE)\n",
        "        return emoji_pattern.sub(r'', inp)\n",
        "\n",
        "    def remove_not_allowed_characters(self, utt):\n",
        "        new_utt = \"\"\n",
        "        for c in utt:\n",
        "            if c in ALLOWED_CHARACTERS:\n",
        "                new_utt += c\n",
        "            elif c.isalpha() or c.isdigit() or not c.strip() or c in PUNCTS or c in EXCEMPTED_CHARACTERS:\n",
        "                if len(new_utt) > 1 and c in PUNCTS and not new_utt[-1].strip():\n",
        "                    new_utt = new_utt[:-1]\n",
        "                new_utt += c\n",
        "            else:\n",
        "                new_utt += \" \"\n",
        "        return new_utt\n",
        "\n",
        "    def post_normalization(self, utt: str, ssml: bool):\n",
        "        if not ssml:\n",
        "            utt = self._remove_successive_puncts([utt])[0]\n",
        "        utt = self._correct_spacing(utt)\n",
        "        while \"  \" in utt:\n",
        "            utt = utt.replace(\"  \", \" \")\n",
        "        return utt\n",
        "\n",
        "# Testing the functions\n",
        "normalizer = TextNormalizer()\n",
        "\n",
        "# Example test for `_is_email`\n",
        "test_utterance_list = [\"hello@example.com\", \"test@sample.com,\", \"<test>\"]\n",
        "print(\"_is_email test result:\", normalizer._is_email(test_utterance_list))\n",
        "\n",
        "# Example test for `_remove_successive_puncts`\n",
        "test_punct_list = [\"Hello,, there!!!\", \"Hi!!\", \"<tag>\"]\n",
        "print(\"_remove_successive_puncts test result:\", normalizer._remove_successive_puncts(test_punct_list))\n",
        "\n",
        "# Example test for `_remove_emojis`\n",
        "test_emoji_input = \"Hello üòä, world! üåç\"\n",
        "print(\"_remove_emojis test result:\", normalizer._remove_emojis(test_emoji_input))\n",
        "\n",
        "# Example test for `remove_not_allowed_characters`\n",
        "test_remove_chars_input = \"Hello@# World! This is a test.\"\n",
        "print(\"remove_not_allowed_characters test result:\", normalizer.remove_not_allowed_characters(test_remove_chars_input))\n",
        "\n",
        "# Example test for `post_normalization`\n",
        "test_post_norm_input = \"This  is  a  test!!\"\n",
        "print(\"post_normalization test result:\", normalizer.post_normalization(test_post_norm_input, ssml=False))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gu3gXYC7T2pa",
        "outputId": "768bfaa8-abcd-4ad9-ac0d-256cc9f7599d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "_is_email test result: ['hello@example.com', 'test@sample.com,', '<test>']\n",
            "_remove_successive_puncts test result: ['Hello, there!', 'Hi!', '<tag>']\n",
            "_remove_emojis test result: Hello , world! \n",
            "remove_not_allowed_characters test result: Hello@# World! This is a test.\n",
            "post_normalization test result: This is a test!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8rInn8Q4T2ml"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dOMRmyvBT2jR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}